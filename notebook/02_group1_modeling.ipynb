{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This chunk of code make it possible to use src functions\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "from DSProject import DSProject\n",
    "import pandas as pd\n",
    "from model import *\n",
    "\n",
    "dsp = DSProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = dsp.read_table(train_dataset + '_processed', 'feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate = dsp.read_table(validate_dataset + '_processed', 'feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = dsp.read_table(score_dataset + '_processed', 'feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y, col_to_drop_numeric, col_to_drop_cat = feature_selection(train, 'group1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_validate, y_validate, _, _ = feature_selection(validate, 'group1', col_to_drop_numeric, col_to_drop_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_score, y_score, _, _ = feature_selection(score, None, col_to_drop_numeric, col_to_drop_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod, result, (x_train, x_test, y_train, y_test) = train_classifier(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def adjusted_threshold_performance(mod, x_test, y_test, df, threshold=0.7):\n",
    "    p = mod.predict_proba(x_test)\n",
    "    y_true = np.array(y_test.tolist(), dtype=int)\n",
    "    y_pred = np.array(p[:, 1] > threshold, dtype=int)\n",
    "    result = classification_report(y_true, y_pred)\n",
    "    print('Group 1 prediction performance after adjusting threshold')\n",
    "    print(result)\n",
    "\n",
    "    # self-cure classification metrics\n",
    "    confm = confusion_matrix(y_true, y_pred)\n",
    "    total_test_pop = y_test.shape[0]\n",
    "    error = confm[0, 1] * 100 / total_test_pop\n",
    "    print(\"Self-cure confusion matrix\")\n",
    "    confusion_table = pd.DataFrame(confm, columns=['Predict 0', 'Predict 1'], index=['Label 0', 'Label 1'])\n",
    "    print(confusion_table)\n",
    "\n",
    "    print(\"Number of people tested\")\n",
    "    print(total_test_pop)\n",
    "    print(\"Number of people predicted to be class 1\")\n",
    "    print(confm[0, 1] + confm[1, 1])\n",
    "    print(\"Percent of people removed\")\n",
    "    print((confm[0, 1] + confm[1, 1]) * 100 / total_test_pop)\n",
    "    print(\"Percentage of mislabeled people\")\n",
    "    print(error)\n",
    "\n",
    "    test = y_test.to_frame()\n",
    "    true_labels = df['Group'].iloc[test.loc[(y_true == 0) & (y_pred == 1)].index]\n",
    "    print(\"Percent of action-roll people who got mistakenly eliminated in this step:\")\n",
    "    print(true_labels.value_counts()[5] * 100 / total_test_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Adjusted-threshold Group1 Results (Testing Set)\")\n",
    "adjusted_threshold_performance(mod['RFC'], x_test, y_test, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Adjusted-threshold Group1 Results (Validation Set)\")\n",
    "adjusted_threshold_performance(mod['RFC'], x_validate, y_validate, validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adjusted_threshold_prediction(mod, x, y, df, threshold=0.7):\n",
    "    p = mod.predict_proba(x)\n",
    "    df['group1_p'] = np.array(p[:, 1])\n",
    "    df['group1_prediction'] = np.array(p[:, 1] > threshold, dtype=int)\n",
    "    nongroup1 = df.loc[df['group1_prediction'] == 0]\n",
    "    return nongroup1, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nongroup1, train = adjusted_threshold_prediction(mod['RFC'], x, y, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsp.write_table(train, train_dataset + '_group1_prediction', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nongroup1, validate = adjusted_threshold_prediction(mod['RFC'], x_validate, y_validate, validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsp.write_table(validate, validate_dataset + '_group1_prediction', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nongroup1, score = adjusted_threshold_prediction(mod['RFC'], x_score, y_score, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsp.write_table(score, score_dataset + '_group1_prediction', 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check group1 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score['group1_prediction'].value_counts()/len(score.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_feature_importance_table(mod['RFC'], x_train, 'feature_importance_group1')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
