{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Action Analytics\n",
    "\n",
    "In order to understand what helps us collect debt, we look at the positive results of collection calls: PTP. We try to understand what influences PTP. Surely customers who have good credit is more likely to make a promise, but can one remove those obvious influence from the call results and focus on the resources we can control such as call time and call agents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This chunk of code make it possible to use src functions\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from dsproject import dsproject\n",
    "\n",
    "dsp = dsproject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsp.data_directory = os.path.join(os.path.dirname(dsp.data_directory), '201510_201610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dsp.read_data('ListAction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "\n",
    "df['date'] = df['FDTACTIONDATE'].apply(lambda d: d.split(\" \")[0])\n",
    "df['time'] = df['FDTACTIONDATE'].apply(lambda d: d.split(\" \")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['date'] = parse_dates(df['date'], '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "db_soc = dsp.read_data('DB_SOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209383"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db_soc.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging dbsoc with list action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_soc['FileDate'] = parse_dates(db_soc['FileDate'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_soc = transform_range_day(db_soc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate unique filedate table\n",
    "unique_filedate = db_soc[['BILLCycle', 'FileDate']].drop_duplicates()\n",
    "\n",
    "unique_filedate['Next_FileDate'] = unique_filedate.groupby('BILLCycle')[['BILLCycle','FileDate']].shift(-1)\n",
    "unique_filedate['Previous_FileDate'] = unique_filedate.groupby('BILLCycle')[['BILLCycle','FileDate']].shift(1)\n",
    "db_soc = db_soc.merge(unique_filedate, on=['BILLCycle', 'FileDate'])\n",
    "\n",
    "unique_filedate = db_soc[['HASH_AR_ID', 'FileDate', 'Previous_FileDate', 'Next_FileDate']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep = df.merge(unique_filedate, on='HASH_AR_ID')\n",
    "prep = prep.loc[(prep['date'] >= prep['FileDate']) & (prep['date'] < prep['Next_FileDate'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FSZCOLLECTORCODE</th>\n",
       "      <th>FSZCOLLECTORSUP</th>\n",
       "      <th>FSZACTIONCODE</th>\n",
       "      <th>FDTACTIONDATE</th>\n",
       "      <th>FResultcategory</th>\n",
       "      <th>FSZRESULTCODE</th>\n",
       "      <th>FDTRESULTDATE</th>\n",
       "      <th>FISRNO</th>\n",
       "      <th>ACTION_DESC</th>\n",
       "      <th>FDTNXTACTDATE</th>\n",
       "      <th>...</th>\n",
       "      <th>NEXTACTION_DESC</th>\n",
       "      <th>ACTIONCOLL</th>\n",
       "      <th>StampDate</th>\n",
       "      <th>HASH_LPM_CST_ID</th>\n",
       "      <th>HASH_AR_ID</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>FileDate</th>\n",
       "      <th>Previous_FileDate</th>\n",
       "      <th>Next_FileDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UPM01SP</td>\n",
       "      <td>DC</td>\n",
       "      <td>15/10/2015 14:47:28</td>\n",
       "      <td>PTP</td>\n",
       "      <td>PTP</td>\n",
       "      <td>20151015</td>\n",
       "      <td>1148803632</td>\n",
       "      <td>Call (Dialer)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UPM0109</td>\n",
       "      <td>20151015</td>\n",
       "      <td>-6799669851245407158</td>\n",
       "      <td>-4749283152832854685</td>\n",
       "      <td>2015-10-15</td>\n",
       "      <td>14:47:28</td>\n",
       "      <td>2015-10-15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UPM01SP</td>\n",
       "      <td>IB</td>\n",
       "      <td>22/10/2015 9:00:28</td>\n",
       "      <td>PTP</td>\n",
       "      <td>PTP</td>\n",
       "      <td>20151022</td>\n",
       "      <td>1157659369</td>\n",
       "      <td>ÅÙ¡¤éÒµÔ´µèÍà¢éÒ Inbound</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INB0209</td>\n",
       "      <td>20151022</td>\n",
       "      <td>-6799669851245407158</td>\n",
       "      <td>-4749283152832854685</td>\n",
       "      <td>2015-10-22</td>\n",
       "      <td>9:00:28</td>\n",
       "      <td>2015-10-15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2015-11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UPF00SP</td>\n",
       "      <td>DC</td>\n",
       "      <td>19/4/2016 15:52:23</td>\n",
       "      <td>PTP</td>\n",
       "      <td>PTP</td>\n",
       "      <td>20160419</td>\n",
       "      <td>1779810434</td>\n",
       "      <td>Call (Dialer)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UPF0105</td>\n",
       "      <td>20160419</td>\n",
       "      <td>-6799669851245407158</td>\n",
       "      <td>-4749283152832854685</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>15:52:23</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>2016-03-16</td>\n",
       "      <td>2016-05-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UPF00SP</td>\n",
       "      <td>DC</td>\n",
       "      <td>27/4/2016 10:01:20</td>\n",
       "      <td>PTP</td>\n",
       "      <td>PTP</td>\n",
       "      <td>20160427</td>\n",
       "      <td>1818612049</td>\n",
       "      <td>Call (Dialer)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UPF0119</td>\n",
       "      <td>20160427</td>\n",
       "      <td>-6799669851245407158</td>\n",
       "      <td>-4749283152832854685</td>\n",
       "      <td>2016-04-27</td>\n",
       "      <td>10:01:20</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>2016-03-16</td>\n",
       "      <td>2016-05-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UPF00SP</td>\n",
       "      <td>DC</td>\n",
       "      <td>9/5/2016 14:36:49</td>\n",
       "      <td>UNS</td>\n",
       "      <td>NOA</td>\n",
       "      <td>20160509</td>\n",
       "      <td>1864598675</td>\n",
       "      <td>Call (Dialer)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UPF0101</td>\n",
       "      <td>20160509</td>\n",
       "      <td>-6799669851245407158</td>\n",
       "      <td>-4749283152832854685</td>\n",
       "      <td>2016-05-09</td>\n",
       "      <td>14:36:49</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>2016-03-16</td>\n",
       "      <td>2016-05-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FSZCOLLECTORCODE FSZCOLLECTORSUP FSZACTIONCODE        FDTACTIONDATE  \\\n",
       "12              NaN         UPM01SP            DC  15/10/2015 14:47:28   \n",
       "18              NaN         UPM01SP            IB   22/10/2015 9:00:28   \n",
       "25              NaN         UPF00SP            DC   19/4/2016 15:52:23   \n",
       "31              NaN         UPF00SP            DC   27/4/2016 10:01:20   \n",
       "37              NaN         UPF00SP            DC    9/5/2016 14:36:49   \n",
       "\n",
       "   FResultcategory FSZRESULTCODE  FDTRESULTDATE      FISRNO  \\\n",
       "12             PTP           PTP       20151015  1148803632   \n",
       "18             PTP           PTP       20151022  1157659369   \n",
       "25             PTP           PTP       20160419  1779810434   \n",
       "31             PTP           PTP       20160427  1818612049   \n",
       "37             UNS           NOA       20160509  1864598675   \n",
       "\n",
       "                 ACTION_DESC FDTNXTACTDATE      ...      NEXTACTION_DESC  \\\n",
       "12             Call (Dialer)           NaN      ...                  NaN   \n",
       "18  ÅÙ¡¤éÒµÔ´µèÍà¢éÒ Inbound           NaN      ...                  NaN   \n",
       "25             Call (Dialer)           NaN      ...                  NaN   \n",
       "31             Call (Dialer)           NaN      ...                  NaN   \n",
       "37             Call (Dialer)           NaN      ...                  NaN   \n",
       "\n",
       "   ACTIONCOLL StampDate      HASH_LPM_CST_ID           HASH_AR_ID       date  \\\n",
       "12    UPM0109  20151015 -6799669851245407158 -4749283152832854685 2015-10-15   \n",
       "18    INB0209  20151022 -6799669851245407158 -4749283152832854685 2015-10-22   \n",
       "25    UPF0105  20160419 -6799669851245407158 -4749283152832854685 2016-04-19   \n",
       "31    UPF0119  20160427 -6799669851245407158 -4749283152832854685 2016-04-27   \n",
       "37    UPF0101  20160509 -6799669851245407158 -4749283152832854685 2016-05-09   \n",
       "\n",
       "        time   FileDate Previous_FileDate Next_FileDate  \n",
       "12  14:47:28 2015-10-15               NaT    2015-11-12  \n",
       "18   9:00:28 2015-10-15               NaT    2015-11-12  \n",
       "25  15:52:23 2016-04-19        2016-03-16    2016-05-13  \n",
       "31  10:01:20 2016-04-19        2016-03-16    2016-05-13  \n",
       "37  14:36:49 2016-04-19        2016-03-16    2016-05-13  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prep = prep.merge(db_soc, on=['HASH_AR_ID', 'FileDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep['Days_past'] = (prep['date']-prep['FileDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep['Days_past'] = prep['Days_past'].apply(lambda d: d.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep['time'] = parse_dates(prep['time'], format='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep['Hours'] = prep['time'].apply(lambda t: t.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge = prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = merge.loc[merge['Range_day'] == 2]\n",
    "b2 = merge.loc[merge['Range_day'] == 3]\n",
    "b3 = merge.loc[merge['Range_day'] == 4]\n",
    "\n",
    "b_list = [b1,b2,b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_columns = ['Hours','ACTIONCOLL', 'PortFolio', 'ProductName', 'BILLCycle', 'Group', 'Card Type New']\n",
    "numeric_columns = ['OSAMT', 'OSPRINCIPLE', 'Days_past']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "labenc = preprocessing.LabelEncoder()\n",
    "\n",
    "cat_code = {}\n",
    "for column in category_columns:\n",
    "    labenc = preprocessing.LabelEncoder()\n",
    "    b1[column] = labenc.fit_transform(b1[column].tolist())\n",
    "    cat_code[column] = labenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_onehot_b1 = pd.get_dummies(b1['FResultcategory'])\n",
    "y_onehot_b2 = pd.get_dummies(b2['FResultcategory'])\n",
    "y_onehot_b3 = pd.get_dummies(b3['FResultcategory'])\n",
    "\n",
    "y_list = [y_onehot_b1,y_onehot_b2,y_onehot_b3]\n",
    "\n",
    "data_num = [(b1[numeric_columns],y_onehot_b1),(b2[numeric_columns],y_onehot_b2),(b3[numeric_columns],y_onehot_b3)]\n",
    "data_nom = [(b1[category_columns],y_onehot_b1),(b2[category_columns],y_onehot_b2),(b3[category_columns],y_onehot_b3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, f_classif\n",
    "import numpy as np\n",
    "\n",
    "def test_feature(x, y, func=f_classif):\n",
    "    f_group1 = func(x, y)\n",
    "    f_group1_table = pd.DataFrame(np.array([list(x.columns), f_group1[0], f_group1[1]]).T,\n",
    "                                  columns=['column_name', 'f_value', 'p_value'])\n",
    "    f_group1_table['f_value'] = f_group1_table['f_value'].astype(np.float64)\n",
    "    f_group1_table['p_value'] = f_group1_table['p_value'].astype(np.float64)\n",
    "    col_to_drop = f_group1_table.loc[f_group1_table['p_value'] > 0.001]['column_name'].tolist()\n",
    "    return f_group1_table, col_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_numeric = []\n",
    "\n",
    "for d in data_num:\n",
    "    b,y = d[0], d[1]\n",
    "    result_table, col_to_drop = test_feature(b, y['PTP'])\n",
    "    df_numeric1 = b.drop(col_to_drop, axis=1)\n",
    "    df_numeric.append(df_numeric1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_nominal = []\n",
    "\n",
    "for d in data_nom:\n",
    "    b,y = d[0], d[1]\n",
    "    result_table, col_to_drop = test_feature(b, y['PTP'],chi2)\n",
    "    df_cat1 = b.drop(col_to_drop, axis=1)\n",
    "    df_nominal.append(df_cat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nominal[0] = pd.get_dummies(df_nominal[0], columns = ['Hours','ACTIONCOLL', 'Group'])\n",
    "df_nominal[1] = pd.get_dummies(df_nominal[1], columns = ['Hours','ACTIONCOLL', 'Group'])\n",
    "df_nominal[2] = pd.get_dummies(df_nominal[2], columns = ['Hours','ACTIONCOLL', 'Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(3):\n",
    "    x.append(pd.concat([df_nominal[i],df_numeric[i]],axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_table, col_to_drop = test_feature(x[0], y_list[0]['PTP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[0] = x[0].drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(x[0].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    yhat = model.predict(x_test)\n",
    "    result = classification_report(y_test, yhat)\n",
    "    print(result)\n",
    "    return model\n",
    "\n",
    "mod = []\n",
    "\n",
    "for i in [0]:\n",
    "    \n",
    "    print (\"Bucket \" + (str)(i+1))\n",
    "    print (\"Spliting training/testing set\")\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x[i], y_list[i]['PTP'], test_size=0.2)\n",
    "    \n",
    "    print (\"Building Model\")\n",
    "\n",
    "    estim = {'RFC': RFC(n_estimators=10, n_jobs=-1, oob_score=True, min_samples_leaf=10, random_state=42)}\n",
    "    mod1 = {}\n",
    "    for key, value in estim.items():\n",
    "        \n",
    "        print (\"Testing Model for Bucket \" + (str)(i+1) + \" using \" + key )\n",
    "        \n",
    "        mod1[key] = test_model(value, x_train, y_train, x_test, y_test)\n",
    "        \n",
    "    mod.append(mod1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "summary = {'Features':x[i].columns, 'Importance':mod[i]['RFC'].feature_importances_}\n",
    "summary = pd.DataFrame(data = summary)\n",
    "print (\"Summary for Bucket \"+ (str)(i+1))\n",
    "print (summary.sort_values('Importance', ascending = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_code['Hours'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_code['ACTIONCOLL'].inverse_transform([356, 361, 359, 358, 360, 395])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
